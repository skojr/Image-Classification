{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KGutuYdgCfn4"
   },
   "outputs": [],
   "source": [
    "# import necessary dependencies\n",
    "import argparse\n",
    "import os, sys\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1745343826574,
     "user": {
      "displayName": "Samuel Obeng",
      "userId": "02945109742445243096"
     },
     "user_tz": 240
    },
    "id": "AP5AE0ggANNC",
    "outputId": "88d9d89d-0255-4a30-8c80-08d2b9d2cc4e"
   },
   "outputs": [],
   "source": [
    "# specify the device for computation\n",
    "#############################################\n",
    "# your code here\n",
    "import torch\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "print(device)\n",
    "!nvidia-smi\n",
    "# import torch\n",
    "# print(torch.__version__)  # Should show PyTorch version\n",
    "# print(torch.cuda.is_available())  # Should return True\n",
    "# print(torch.version.cuda)  # Should return 11.8\n",
    "# print(torch.cuda.get_device_name(0))  # Should return your GPU name\n",
    "\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUmUvMbf9fsw"
   },
   "source": [
    "# Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OcQPv0u-6vtJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "from torchvision import models\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        weights = models.ResNet18_Weights.IMAGENET1K_V1  # or ResNet50_Weights.DEFAULT\n",
    "        resnet = models.resnet18(weights=weights)\n",
    "\n",
    "        # Modify first conv and remove maxpool for small CIFAR-10 images\n",
    "        resnet.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        resnet.maxpool = nn.Identity()\n",
    "\n",
    "        # Remove final FC layer, keep everything else\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])  # [B, 512, 1, 1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)          # [B, 512, 1, 1]\n",
    "        return torch.flatten(x, 1)  # Flatten ‚Üí [B, 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bNO05NvI65iq"
   },
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, in_dim=512, hidden_dim=512, out_dim=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "          nn.Linear(in_dim, hidden_dim),\n",
    "          nn.BatchNorm1d(hidden_dim),\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.Linear(hidden_dim, hidden_dim),\n",
    "          nn.BatchNorm1d(hidden_dim),\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.Linear(hidden_dim, out_dim)\n",
    "      )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "skVi-4yM8XA5"
   },
   "outputs": [],
   "source": [
    "class RotationHead(nn.Module):\n",
    "    def __init__(self, in_dim=512, num_classes=4):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mtgNS4HK8VZ_"
   },
   "outputs": [],
   "source": [
    "class SimCLRRotNetModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.projection_head = ProjectionHead()\n",
    "        self.rotation_head = RotationHead()\n",
    "\n",
    "    def forward(self, x, task='simclr'):\n",
    "        features = self.encoder(x)\n",
    "        if task == 'simclr':\n",
    "            return self.projection_head(features)\n",
    "        elif task == 'rotnet':\n",
    "            return self.rotation_head(features)\n",
    "        else:\n",
    "            raise ValueError(\"Task must be 'simclr' or 'rotnet'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1222,
     "status": "ok",
     "timestamp": 1745343833214,
     "user": {
      "displayName": "Samuel Obeng",
      "userId": "02945109742445243096"
     },
     "user_tz": 240
    },
    "id": "yztSoVjV6ML9",
    "outputId": "9f5495c6-4dd5-4976-e45d-ea6efd2321d0"
   },
   "outputs": [],
   "source": [
    "def test_encoder():\n",
    "    model = Encoder()\n",
    "    dummy_input = torch.randn(2, 3, 32, 32)  # CIFAR-10 shaped input\n",
    "    output = model(dummy_input)\n",
    "    assert output.shape[0] == 2, \"Batch size mismatch\"\n",
    "    print(f\"‚úÖ Encoder output shape: {output.shape}\")\n",
    "\n",
    "def test_projection_head():\n",
    "    head = ProjectionHead(in_dim=512, out_dim=128)\n",
    "    dummy_feat = torch.randn(2, 512)\n",
    "    output = head(dummy_feat)\n",
    "    assert output.shape == (2, 128), \"Projection head output mismatch\"\n",
    "    print(f\"‚úÖ ProjectionHead output shape: {output.shape}\")\n",
    "\n",
    "def test_rotation_head():\n",
    "    head = RotationHead(in_dim=512, num_classes=4)\n",
    "    dummy_feat = torch.randn(2, 512)\n",
    "    output = head(dummy_feat)\n",
    "    assert output.shape == (2, 4), \"Rotation head output mismatch\"\n",
    "    print(f\"‚úÖ RotationHead output shape: {output.shape}\")\n",
    "\n",
    "def test_full_model():\n",
    "    model = SimCLRRotNetModel()\n",
    "    dummy_input = torch.randn(2, 3, 32, 32)\n",
    "\n",
    "    # Test SimCLR head\n",
    "    out_simclr = model(dummy_input, task='simclr')\n",
    "    assert out_simclr.shape[0] == 2, \"SimCLR batch size mismatch\"\n",
    "    print(f\"‚úÖ SimCLR head output: {out_simclr.shape}\")\n",
    "\n",
    "    # Test RotNet head\n",
    "    out_rotnet = model(dummy_input, task='rotnet')\n",
    "    assert out_rotnet.shape == (2, 4), \"RotNet output mismatch\"\n",
    "    print(f\"‚úÖ RotNet head output: {out_rotnet.shape}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_encoder()\n",
    "    test_projection_head()\n",
    "    test_rotation_head()\n",
    "    test_full_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mh1dbJiV67TW"
   },
   "source": [
    "## Data Preprocessing and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eMHjU8ni66j7"
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "simclr_transform = T.Compose([\n",
    "    T.RandomResizedCrop(size=32, scale=(0.5, 1.0)),  # lighter resize crop\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.ColorJitter(0.5, 0.5, 0.5, 0.2),  # strong but lighter\n",
    "    T.RandomGrayscale(p=0.2),\n",
    "    T.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),  # lighter blur for 32x32\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=(0.4914, 0.4822, 0.4465),  std=(0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "rotnet_base_transform = T.ToTensor()  # no spatial transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6482,
     "status": "ok",
     "timestamp": 1745343839699,
     "user": {
      "displayName": "Samuel Obeng",
      "userId": "02945109742445243096"
     },
     "user_tz": 240
    },
    "id": "nVcibElh8NzX",
    "outputId": "1b8b7dda-c630-4bef-dbe5-e191fa971be5"
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_set = CIFAR10(root='./data', train=True, download=True, transform=simclr_transform)\n",
    "train_loader = DataLoader(train_set, batch_size=256, shuffle=True, num_workers=2, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "atW1tdzP8kKC"
   },
   "outputs": [],
   "source": [
    "def rotate_batch(x):\n",
    "    rotations = []\n",
    "    labels = []\n",
    "    for k in range(4):\n",
    "        rotated = torch.rot90(x, k=k, dims=[2, 3])\n",
    "        rotations.append(rotated)\n",
    "        labels += [k] * x.size(0)\n",
    "    return torch.cat(rotations), torch.tensor(labels, device=x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7EFamVW8hFl"
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "class SimCLRRotNetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_dataset, simclr_transform, rotnet_transform):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.simclr_transform = simclr_transform\n",
    "        self.rotnet_transform = rotnet_transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, _ = self.base_dataset[index]\n",
    "\n",
    "        # SimCLR views\n",
    "        x_i = self.simclr_transform(img)\n",
    "        x_j = self.simclr_transform(img)\n",
    "\n",
    "        # RotNet base (apply rotations in batch later)\n",
    "        x_rot = self.rotnet_transform(img)\n",
    "\n",
    "        return x_i, x_j, x_rot\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "deC6lUgX_qfZ"
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "base_dataset = CIFAR10(root='./data', train=True, download=True)\n",
    "multi_view_dataset = SimCLRRotNetDataset(base_dataset, simclr_transform, rotnet_base_transform)\n",
    "\n",
    "train_loader = DataLoader(multi_view_dataset, batch_size=256, shuffle=True, num_workers=2, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ul83aYwzA4BI"
   },
   "source": [
    "## Training Setup and Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rPtMgCqlA_PK"
   },
   "outputs": [],
   "source": [
    "# SimCLR Loss Function\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def nt_xent_loss(z_i, z_j, temperature=0.5):\n",
    "    z_i = F.normalize(z_i, dim=1)\n",
    "    z_j = F.normalize(z_j, dim=1)\n",
    "    representations = torch.cat([z_i, z_j], dim=0)  # [2N, D]\n",
    "\n",
    "    similarity_matrix = torch.matmul(representations, representations.T)  # [2N, 2N]\n",
    "    batch_size = z_i.size(0)\n",
    "    labels = torch.arange(batch_size, device=z_i.device)\n",
    "    labels = torch.cat([labels, labels], dim=0)\n",
    "\n",
    "    # Mask out self-similarity\n",
    "    mask = torch.eye(2 * batch_size, device=z_i.device).bool()\n",
    "    similarity_matrix = similarity_matrix[~mask].view(2 * batch_size, -1)\n",
    "\n",
    "    positives = torch.sum(z_i * z_j, dim=1)\n",
    "    positives = torch.cat([positives, positives], dim=0)\n",
    "\n",
    "    logits = similarity_matrix / temperature\n",
    "    loss = F.cross_entropy(logits, labels)\n",
    "    return loss\n",
    "\n",
    "# RotNet Loss Function\n",
    "rotation_loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3522,
     "status": "ok",
     "timestamp": 1745338679683,
     "user": {
      "displayName": "Samuel Obeng",
      "userId": "02945109742445243096"
     },
     "user_tz": 240
    },
    "id": "CCpqi0e06kNG",
    "outputId": "d6c26ebf-8b13-4f62-fa04-623e1e0e4d6c"
   },
   "outputs": [],
   "source": [
    "pip install warmup_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7787,
     "status": "ok",
     "timestamp": 1745297905446,
     "user": {
      "displayName": "Samuel Obeng",
      "userId": "02945109742445243096"
     },
     "user_tz": 240
    },
    "id": "ChIHEoCuCkZL",
    "outputId": "19516624-62c3-4123-c9ab-a3bff99ad9fa"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "\n",
    "CHECKPOINT_FOLDER = '/content/drive/My Drive/ECE661/Project/hybrid_model/training_models/cosine_warmup_scheduler'\n",
    "LEARNING_RATE = 0.05\n",
    "WARMUP_EPOCHS = 10\n",
    "NUM_EPOCHS = 200\n",
    "TEMPERATURE = 0.2\n",
    "scaler = GradScaler(device='cuda')\n",
    "start_epoch = 0\n",
    "\n",
    "# üß† Define model and optimizer before loading checkpoint\n",
    "model = SimCLRRotNetModel().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "# Set up schedulers\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=5, eta_min=0.001)\n",
    "gradual_warmup = GradualWarmupScheduler(optimizer, multiplier=10, total_epoch=WARMUP_EPOCHS, after_scheduler=scheduler)\n",
    "\n",
    "# üîÅ Try to resume from a checkpoint\n",
    "for epoch in range(200, -10, -10):\n",
    "    ckpt_name = f\"SimCLR_RotNet_epoch{epoch}.pth\"\n",
    "    ckpt_path = os.path.join(CHECKPOINT_FOLDER, ckpt_name)\n",
    "    if os.path.exists(ckpt_path):\n",
    "        checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        scheduler.last_epoch = start_epoch - 1\n",
    "        print(f\"üîÅ Resumed from checkpoint: {ckpt_path} (epoch {start_epoch})\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"No checkpoint found at {epoch}. Starting training from scratch.\")\n",
    "\n",
    "# # ‚è© Step scheduler forward to match resumed epoch\n",
    "# scheduler.step()\n",
    "\n",
    "# üîÅ Training loop\n",
    "for epoch in range(start_epoch, NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_simclr_loss = 0\n",
    "    total_rotnet_loss = 0\n",
    "    num_batches = len(train_loader)\n",
    "    start_time = time.time()\n",
    "\n",
    "    for step, (x_i, x_j, x_rot) in enumerate(train_loader, 1):\n",
    "        x_i, x_j, x_rot = x_i.to(device), x_j.to(device), x_rot.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast(device_type='cuda'):\n",
    "            z_i = model(x_i, task='simclr')\n",
    "            z_j = model(x_j, task='simclr')\n",
    "            loss_simclr = nt_xent_loss(z_i, z_j, temperature=TEMPERATURE)\n",
    "\n",
    "            x_rot_batch, rot_labels = rotate_batch(x_rot)\n",
    "            rot_logits = model(x_rot_batch, task='rotnet')\n",
    "            loss_rotnet = rotation_loss_fn(rot_logits, rot_labels)\n",
    "\n",
    "            loss = loss_simclr + ROT_LOSS_WEIGHT * loss_rotnet\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_simclr_loss += loss_simclr.item()\n",
    "        total_rotnet_loss += loss_rotnet.item()\n",
    "\n",
    "        if step % 20 == 0 or step == num_batches:\n",
    "            print(f\"[Step {step}/{num_batches}] Total: {loss.item():.4f} | SimCLR: {loss_simclr.item():.4f} | RotNet: {loss_rotnet.item():.4f}\")\n",
    "\n",
    "    avg_total_loss = total_loss / num_batches\n",
    "    avg_simclr_loss = total_simclr_loss / num_batches\n",
    "    avg_rotnet_loss = total_rotnet_loss / num_batches\n",
    "    epoch_time = time.time() - start_time\n",
    "\n",
    "    scheduler.step()  # üîÅ Update learning rate\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}/{NUM_EPOCHS}] Avg Total: {avg_total_loss:.4f} | \"\n",
    "          f\"SimCLR: {avg_simclr_loss:.4f} | \"\n",
    "          f\"RotNet: {avg_rotnet_loss:.4f} | ‚è± Time: {epoch_time:.2f}s\\n\"\n",
    "          f\"Learning Rate (Epoch {epoch+1}): {current_lr:.6f}\\n\")\n",
    "\n",
    "    # üíæ Save checkpoint every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scaler_state_dict': scaler.state_dict(),\n",
    "            'avg_total_loss': avg_total_loss,\n",
    "            'avg_simclr_loss': avg_simclr_loss,\n",
    "            'avg_rotnet_loss': avg_rotnet_loss\n",
    "        }\n",
    "        save_path = os.path.join(CHECKPOINT_FOLDER, f\"SimCLR_RotNet_epoch{epoch+1}.pth\")\n",
    "        os.makedirs(CHECKPOINT_FOLDER, exist_ok=True)\n",
    "        torch.save(checkpoint, save_path)\n",
    "        print(f\"‚úÖ Saved checkpoint and losses at: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wF7hmT9ejLQj"
   },
   "source": [
    "# Updating SimCLR+RotNet Training Loop\n",
    "### Reduce RotNet Weight to 0.2, Use only Cosine Scheduler, Freeze Rotation Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h_jke2WLh9Ke"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "def train_rotnet_simclr(\n",
    "    model,\n",
    "    optimizer,\n",
    "    scaler,\n",
    "    train_loader,\n",
    "    rotate_batch,\n",
    "    rotation_loss_fn,\n",
    "    nt_xent_loss,\n",
    "    gradual_warmup,\n",
    "    device,\n",
    "    BATCH_SIZE,\n",
    "    CHECKPOINT_FOLDER,\n",
    "    NUM_EPOCHS,\n",
    "    TEMPERATURE\n",
    "):\n",
    "  # üîÅ Try to resume from a checkpoint\n",
    "  for epoch in range(100, -10, -10):\n",
    "      ckpt_name = f\"SimCLR_RotNet_bs{BATCH_SIZE}_epoch{epoch}.pth\"\n",
    "      ckpt_path = os.path.join(CHECKPOINT_FOLDER, ckpt_name)\n",
    "      if os.path.exists(ckpt_path):\n",
    "          checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "          model.load_state_dict(checkpoint['model_state_dict'])\n",
    "          optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "          scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "          start_epoch = checkpoint['epoch']\n",
    "          gradual_warmup.last_epoch = start_epoch - 1\n",
    "          print(f\"Resumed from checkpoint: {ckpt_path} (epoch {start_epoch})\")\n",
    "          break\n",
    "      else:\n",
    "          print(f\"No checkpoint found at {epoch}. Starting training from scratch.\")\n",
    "          start_epoch=0\n",
    "\n",
    "  if start_epoch >= 25:\n",
    "      for param in model.rotation_head.parameters():\n",
    "          param.requires_grad = False\n",
    "\n",
    "  # üîÅ Training loop\n",
    "  print(f\"Begin Training for Batch Size={BATCH_SIZE}\")\n",
    "  for epoch in range(start_epoch, NUM_EPOCHS):\n",
    "      model.train()\n",
    "      total_loss = 0\n",
    "      total_simclr_loss = 0\n",
    "      total_rotnet_loss = 0\n",
    "      num_batches = len(train_loader)\n",
    "      start_time = time.time()\n",
    "\n",
    "      if epoch < 25:\n",
    "        current_rot_weight = 0.5 * (1 - epoch / 25)\n",
    "        compute_rot = True\n",
    "      else:\n",
    "        compute_rot = False\n",
    "        current_rot_weight = 0.0\n",
    "        for param in model.rotation_head.parameters():\n",
    "          param.requires_grad = False\n",
    "\n",
    "      for step, (x_i, x_j, x_rot) in enumerate(train_loader, 1):\n",
    "          x_i, x_j, x_rot = x_i.to(device), x_j.to(device), x_rot.to(device)\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          with autocast(device_type='cuda'):\n",
    "              z_i = model(x_i, task='simclr')\n",
    "              z_j = model(x_j, task='simclr')\n",
    "              loss_simclr = nt_xent_loss(z_i, z_j, temperature=TEMPERATURE)\n",
    "\n",
    "              if compute_rot:\n",
    "                x_rot_batch, rot_labels = rotate_batch(x_rot)\n",
    "                rot_logits = model(x_rot_batch, task='rotnet')\n",
    "                loss_rotnet = rotation_loss_fn(rot_logits, rot_labels)\n",
    "              else:\n",
    "                  loss_rotnet = torch.tensor(0.0, device=device)\n",
    "\n",
    "              loss = loss_simclr + current_rot_weight * loss_rotnet\n",
    "\n",
    "          scaler.scale(loss).backward()\n",
    "          scaler.step(optimizer)\n",
    "          scaler.update()\n",
    "\n",
    "          total_loss += loss.item()\n",
    "          total_simclr_loss += loss_simclr.item()\n",
    "          total_rotnet_loss += loss_rotnet.item()\n",
    "\n",
    "          if step % 20 == 0 or step == num_batches:\n",
    "              print(f\"[Step {step}/{num_batches}] Total: {loss.item():.4f} | SimCLR: {loss_simclr.item():.4f} | RotNet: {loss_rotnet.item():.4f}\")\n",
    "\n",
    "      avg_total_loss = total_loss / num_batches\n",
    "      avg_simclr_loss = total_simclr_loss / num_batches\n",
    "      avg_rotnet_loss = total_rotnet_loss / num_batches\n",
    "      epoch_time = time.time() - start_time\n",
    "\n",
    "      gradual_warmup.step()  # üîÅ Update learning rate\n",
    "      current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "      print(f\"[Epoch {epoch+1}/{NUM_EPOCHS}] Avg Total: {avg_total_loss:.4f} | \"\n",
    "            f\"SimCLR: {avg_simclr_loss:.4f} | \"\n",
    "            f\"RotNet: {avg_rotnet_loss:.4f} | Time: {epoch_time:.2f}s\\n\"\n",
    "            f\"Learning Rate (Epoch {epoch+1}): {current_lr:.6f}\\n\")\n",
    "\n",
    "      # Save checkpoint every 10 epochs\n",
    "      if (epoch + 1) % 10 == 0:\n",
    "          checkpoint = {\n",
    "              'epoch': epoch + 1,\n",
    "              'model_state_dict': model.state_dict(),\n",
    "              'optimizer_state_dict': optimizer.state_dict(),\n",
    "              'scaler_state_dict': scaler.state_dict(),\n",
    "              'avg_total_loss': avg_total_loss,\n",
    "              'avg_simclr_loss': avg_simclr_loss,\n",
    "              'avg_rotnet_loss': avg_rotnet_loss\n",
    "          }\n",
    "          save_path = os.path.join(CHECKPOINT_FOLDER, f\"SimCLR_RotNet_bs{BATCH_SIZE}_epoch{epoch+1}.pth\")\n",
    "          os.makedirs(CHECKPOINT_FOLDER, exist_ok=True)\n",
    "          torch.save(checkpoint, save_path)\n",
    "          print(f\"‚úÖ Saved checkpoint and losses at: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qfe6coxgHate"
   },
   "outputs": [],
   "source": [
    "# SETUP\n",
    "import torch\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "\n",
    "CHECKPOINT_FOLDER = '/content/drive/My Drive/ECE661/Project/hybrid_model/training_models/stronger_augmentation'\n",
    "LEARNING_RATE = 0.025\n",
    "ETA_MIN = 0.0125\n",
    "WARMUP_EPOCHS = 10\n",
    "NUM_EPOCHS = 100\n",
    "scaler = GradScaler(device='cuda')\n",
    "start_epoch = 0\n",
    "WEIGHT_DECAY = 1e-4\n",
    "TEMPERATURE = 0.2 # Try 0.2, makes contrastive learning more sharp\n",
    "\n",
    "# üß† Define model and optimizer before loading checkpoint\n",
    "model = SimCLRRotNetModel().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "# Unfreeze encoder\n",
    "for param in model.encoder.parameters():\n",
    "    assert param.requires_grad == True\n",
    "\n",
    "# üîí Freeze BatchNorm layers in the encoder\n",
    "for m in model.encoder.modules():\n",
    "    if isinstance(m, torch.nn.BatchNorm2d):\n",
    "        m.eval()\n",
    "\n",
    "# Cosine Annealing after warmup: from 0.025 ‚Üí 0.0125\n",
    "cosine_scheduler = CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=100,\n",
    "    eta_min=ETA_MIN\n",
    ")\n",
    "\n",
    "# Warmup scheduler: ramps up from 0.0025 ‚Üí 0.025 over 10 epochs, then hands off to cosine\n",
    "gradual_warmup = GradualWarmupScheduler(\n",
    "    optimizer,\n",
    "    multiplier=1.0,  # keep LR at base (0.025) after warmup\n",
    "    total_epoch=WARMUP_EPOCHS,\n",
    "    after_scheduler=cosine_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kp-T7-c3LtIP"
   },
   "source": [
    "# Train Model on Batch Size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1745339488721,
     "user": {
      "displayName": "Samuel Obeng",
      "userId": "02945109742445243096"
     },
     "user_tz": 240
    },
    "id": "ejLp_7unSPpP",
    "outputId": "32f5aa07-f6aa-4275-b7fb-e9a41743e2ad"
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "BATCH_SIZE = 256\n",
    "CHECKPOINT_FOLDER = '/content/drive/My Drive/ECE661/Project/hybrid_model/training_models/stronger_augmentation'\n",
    "\n",
    "os.listdir(CHECKPOINT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2780889,
     "status": "error",
     "timestamp": 1745343549795,
     "user": {
      "displayName": "Samuel Obeng",
      "userId": "02945109742445243096"
     },
     "user_tz": 240
    },
    "id": "P0O2LU2ZIkYR",
    "outputId": "0c660cb3-a45a-40e9-ef55-e9838b3e920c"
   },
   "outputs": [],
   "source": [
    "# Batch Size = 64\n",
    "BATCH_SIZE = 64\n",
    "CHECKPOINT_FOLDER = '/content/drive/My Drive/ECE661/Project/hybrid_model/training_models/stronger_augmentation'\n",
    "train_rotnet_simclr(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    scaler=scaler,\n",
    "    train_loader=train_loader,  # make sure you have defined this elsewhere\n",
    "    rotate_batch=rotate_batch,  # function that returns rotated batch and labels\n",
    "    rotation_loss_fn=rotation_loss_fn,  # e.g., nn.CrossEntropyLoss()\n",
    "    nt_xent_loss=nt_xent_loss,  # your contrastive loss function\n",
    "    gradual_warmup=gradual_warmup,\n",
    "    device='cuda',\n",
    "    BATCH_SIZE=BATCH_SIZE,\n",
    "    CHECKPOINT_FOLDER=CHECKPOINT_FOLDER,\n",
    "    NUM_EPOCHS=100,\n",
    "    TEMPERATURE=TEMPERATURE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scZp2EsfKYaZ"
   },
   "outputs": [],
   "source": [
    "# Batch Size = 128\n",
    "BATCH_SIZE = 128\n",
    "CHECKPOINT_FOLDER = '/content/drive/My Drive/ECE661/Project/hybrid_model/training_models/stronger_augmentation'\n",
    "train_rotnet_simclr(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    scaler=scaler,\n",
    "    train_loader=train_loader,  # make sure you have defined this elsewhere\n",
    "    rotate_batch=rotate_batch,  # function that returns rotated batch and labels\n",
    "    rotation_loss_fn=rotation_loss_fn,  # e.g., nn.CrossEntropyLoss()\n",
    "    nt_xent_loss=nt_xent_loss,  # your contrastive loss function\n",
    "    gradual_warmup=gradual_warmup,\n",
    "    device='cuda',\n",
    "    BATCH_SIZE=BATCH_SIZE,\n",
    "    CHECKPOINT_FOLDER=CHECKPOINT_FOLDER,\n",
    "    NUM_EPOCHS=100,\n",
    "    TEMPERATURE=TEMPERATURE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNsL0bLOPfaU"
   },
   "source": [
    "# Linear Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nDbP6towcg-J"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "\n",
    "def train_linear_classifier(model_name, encoder, train_features, train_labels, test_features, test_labels,\n",
    "                            batch_size=256, num_epochs=20, lr=0.1):\n",
    "    \"\"\"\n",
    "    Trains a linear classifier on frozen features.\n",
    "    \"\"\"\n",
    "    print(f\"[{model_name}] Evaluating...\")\n",
    "\n",
    "    # Freeze encoder\n",
    "    encoder.eval()\n",
    "    for param in encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Create classifier and move to device\n",
    "    classifier = nn.Linear(train_features.shape[1], 10).to(device)\n",
    "\n",
    "    # Optimizer and loss\n",
    "    optimizer = optim.SGD(classifier.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(TensorDataset(train_features, train_labels), batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(TensorDataset(test_features, test_labels), batch_size=batch_size)\n",
    "\n",
    "    # Training loop\n",
    "    classifier.train()\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        epoch_loss = 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = classifier(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "    # Evaluation\n",
    "    classifier.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = classifier(x)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    acc = correct / total\n",
    "    print(f\"[{model_name}] Final Test Accuracy: {acc * 100:.2f}%\\n\")\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OeoIz-S4TyK-"
   },
   "outputs": [],
   "source": [
    "from torch.amp import autocast\n",
    "def getEncoder(model):\n",
    "    return model.encoder.eval()\n",
    "\n",
    "def extractFeatures(dataloader, encoder):\n",
    "    encoder.eval()\n",
    "    features, labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in dataloader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            with autocast(device_type=\"cuda\"):  # Mixed precision context\n",
    "                outputs = encoder(images)\n",
    "\n",
    "            outputs = outputs.float().cpu()  # Convert back to full float32, then move to CPU\n",
    "            features.append(outputs)\n",
    "            labels.append(targets)\n",
    "\n",
    "    return torch.cat(features), torch.cat(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RrNMrNG2T3jj"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "\n",
    "\n",
    "DATA_ROOT = '/content/data'\n",
    "transform_eval = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "trainDataset = torchvision.datasets.CIFAR10(root=DATA_ROOT, train=True, download=True, transform=transform_eval)\n",
    "testDataset = torchvision.datasets.CIFAR10(root=DATA_ROOT, train=False, download=True, transform=transform_eval)\n",
    "train_loader_eval = DataLoader(trainDataset, batch_size=256, shuffle=False, num_workers=2)\n",
    "test_loader_eval = DataLoader(testDataset, batch_size=256, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QbtkAP_yKq2_"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch  # Make sure torch is imported\n",
    "\n",
    "BATCH_SIZES = [256]\n",
    "EPOCHS = np.arange(20, 201, 20)\n",
    "\n",
    "def linear_eval(train_loader_eval, test_loader_eval, percent_labeled, CHECKPOINT_FOLDER):\n",
    "    RESULTS_FOLDER = '/content/drive/My Drive/ECE661/Project/hybrid_model/linear_evals'\n",
    "    os.makedirs(RESULTS_FOLDER, exist_ok=True)  # Ensure results folder exists\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for bs in BATCH_SIZES:\n",
    "        print(\"=\" * 150)\n",
    "        print(f\"Starting Linear Evaluation for Batch Size {bs}...\\n\")\n",
    "        for epoch in EPOCHS:\n",
    "            # 1. Load Pretrained Model Checkpoint\n",
    "            checkpoint_name = f\"SimCLR_RotNet_bs{bs}_epoch{epoch}.pth\"\n",
    "            checkpoint_path = os.path.join(\n",
    "                CHECKPOINT_FOLDER,\n",
    "                f\"SimCLR_RotNet_bs{bs}_epoch{epoch}.pth\"\n",
    "            )\n",
    "            if not os.path.exists(checkpoint_path):\n",
    "                print(f\"‚ö†Ô∏è Checkpoint not found at {checkpoint_path}\")\n",
    "                continue\n",
    "\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "            model = SimCLRRotNetModel().to(device)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "            # 2. Extract and Freeze Encoder\n",
    "            encoder = model.encoder.to(device)\n",
    "            encoder.eval()\n",
    "            for param in encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            # 3. Extract features\n",
    "            train_features, train_labels = extractFeatures(train_loader_eval, encoder)\n",
    "            test_features, test_labels = extractFeatures(test_loader_eval, encoder)\n",
    "\n",
    "            # 4. Train linear classifier\n",
    "            accuracy = train_linear_classifier(\n",
    "                checkpoint_name,\n",
    "                encoder,\n",
    "                train_features, train_labels,\n",
    "                test_features, test_labels,\n",
    "                batch_size=256,\n",
    "                num_epochs=20,\n",
    "                lr=0.1\n",
    "            )\n",
    "\n",
    "            result = {\n",
    "                \"epoch\": epoch,\n",
    "                \"batch_size\": bs,\n",
    "                \"acc\": accuracy\n",
    "            }\n",
    "            all_results.append(result)\n",
    "\n",
    "    if all_results:\n",
    "        df = pd.DataFrame(all_results)\n",
    "        save_path = os.path.join(RESULTS_FOLDER, f\"hybrid_model_{percent_labeled}percent_data_accuracy_summary.csv\")\n",
    "        df.to_csv(save_path, index=False)\n",
    "        print(f\"All results saved to {save_path}\")\n",
    "    else:\n",
    "        print(\"No results to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 219016,
     "status": "ok",
     "timestamp": 1745345993892,
     "user": {
      "displayName": "Samuel Obeng",
      "userId": "02945109742445243096"
     },
     "user_tz": 240
    },
    "id": "fKLVs1NiXvP-",
    "outputId": "0de083d8-5782-4821-9156-de2c07c17b6c"
   },
   "outputs": [],
   "source": [
    "CHECKPOINT_FOLDER = '/content/drive/My Drive/ECE661/Project/hybrid_model/training_models/stronger_augmentation'\n",
    "linear_eval(train_loader_eval, test_loader_eval, 100, CHECKPOINT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "executionInfo": {
     "elapsed": 509,
     "status": "ok",
     "timestamp": 1745382738104,
     "user": {
      "displayName": "Tony Nunn",
      "userId": "11350351903651270957"
     },
     "user_tz": 240
    },
    "id": "PHw6bxhmvBcW",
    "outputId": "d9bdd805-1e27-4600-a8fb-0d93df91581f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load all results\n",
    "hybrid_df = pd.read_csv(\"/content/drive/My Drive/ECE 661/Project/hybrid_model/linear_evals/hybrid_model_100percent_data_accuracy_summary.csv\")\n",
    "simclr_df = pd.read_csv(\"/content/drive/My Drive/ECE 661/Project/linear_evals/LRset2_100percent_labeled_data_accuracy_summary.csv\")\n",
    "rotnet_df = pd.read_csv(\"/content/drive/My Drive/ECE 661/Project/final_rotnet_eval_resultsCSV.csv\")\n",
    "\n",
    "# Filter for batch_size = 256\n",
    "simclr_df = simclr_df[simclr_df[\"batch_size\"] == 256]\n",
    "rotnet_df = rotnet_df[rotnet_df[\"batch_size\"] == 256]\n",
    "\n",
    "# Create consistent x positions\n",
    "x = np.arange(len(hybrid_df[\"epoch\"]))\n",
    "bar_width = 0.25\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x - bar_width, hybrid_df[\"acc\"], width=bar_width, label=\"Hybrid\")\n",
    "plt.bar(x, simclr_df[\"avg_accuracy\"], width=bar_width, label=\"SimCLR\")\n",
    "plt.bar(x + bar_width, rotnet_df[\"accuracy\"], width=bar_width, label=\"RotNet\")\n",
    "\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Top 1 Accuracy\")\n",
    "plt.title(\"Top 1 Accuracy vs Epoch (Batch Size = 256)\")\n",
    "plt.xticks(x, hybrid_df[\"epoch\"])\n",
    "plt.ylim(0.6, 0.9)\n",
    "plt.grid(axis='y')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"figure_hybridComp2.svg\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23274,
     "status": "ok",
     "timestamp": 1745340675284,
     "user": {
      "displayName": "Samuel Obeng",
      "userId": "02945109742445243096"
     },
     "user_tz": 240
    },
    "id": "OjYfVLUjTwgX",
    "outputId": "cb88178d-4008-4dbe-f2f7-47a62ca1323d"
   },
   "outputs": [],
   "source": [
    "# 1. Load Pretrained Model Checkpoint\n",
    "checkpoint_path = '/content/drive/My Drive/ECE661/Project/hybrid_model/training_models/stronger_augmentation/SimCLR_RotNet_epoch90.pth'\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "model = SimCLRRotNetModel().to(device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# 2. Extract and Freeze Encoder\n",
    "encoder = model.encoder.to(device)\n",
    "encoder.eval()\n",
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False  # freeze encoder\n",
    "\n",
    "# Get features\n",
    "train_features, train_labels = extractFeatures(train_loader_eval, encoder)\n",
    "test_features, test_labels = extractFeatures(test_loader_eval, encoder)\n",
    "\n",
    "# Run linear eval\n",
    "accuracy = train_linear_classifier(\"SimCLR+RotNet\", encoder,\n",
    "                                   train_features, train_labels,\n",
    "                                   test_features, test_labels,\n",
    "                                   batch_size=256, num_epochs=20, lr=0.1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
